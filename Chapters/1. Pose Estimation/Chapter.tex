\section{Problemas}

\subsection{Estimación de Pose en Humanos}

La tarea de \textit{Estimación de Pose en Humanos} (HPE por sis siglas en inglés) ha sido uno de los
tópicos de gran importancia en
el campo de Visión por Computadora. Debido a la búsqueda de automatización y entendimiento de
diversas actividades humanas, sus utilidades causan impacto directo en las implementaciones
tecnológicas del mundo real, tales como, la predicción de intención (vigilancia), sistemas de
autónomos y de asistencia en la conducción automóviles, animación, simulaciones,
interacción Humano-Computadora (HCI), realidad virtual
aumentada (VR y AR), videojuegos, salud o asistencia médica o hasta análisis de movimiento en
deportes. La tarea de \textit{Estimación de Pose} no solo se limita a el cuerpo
humano, también, puede ser empleado en objetos como carros o animales, vease la imagen \ref{fig:PE-track}.

Con el crecimiento acelerado de \textit{Aprendizaje Profundo} en los últimos años gracias a las
capacidades actuales de potencia de cómputo los métodos basados bajo este enfoque han sobrepasado
a las métodos tradicionales, sin embargo aún existen distintos problemas y retos que siguen presentes
como la oclusión y la ambiguedad de los datos o la dificultad de su obtención para realizar
entrenamientos.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.4 \textwidth]{Chapters/1. Pose Estimation/figures/openpifpaf.png}
    \caption{OpenPifPaf: Escena del mundo real desde la perspectiva de un carro autónomo. Todos los actores
             son detectados y seguidos, esto incluye a las personas, el carro y el perro. \cite{DBLP:journals/corr/abs-2103-02440}}
    \label{fig:PE-track}
\end{figure}


El problema de \textit{Estimación de Pose Humanos} consiste en predecir las partes del cuerpo o las
posiciones de las articulaciones de una persona a través de una imagen, video. Este problema ha sido
cuidadosamente estudiado a lo largo de los años y diversas recopilaciones de investigaciones han sido escritas.
En la tabla \ref{Tab:hpe-survey} se resumen algunas de las más recientes y que describen dos formas
generales de abordar el problema. La primera de ellas la \quotes{Tradicionalista}, cuyos métodos
usan enfoques clásicos de visión por computadora o la segunda basada en técnicas de aprendizaje
profundo que involucran comúnmente modelos convolucionales. El trabajo realizado en esta tesis está
basado en el segundo método, usando técnicas de aprendizaje profundo y modelos actuales capaces
de capturar información temporal, específicamente enfocado en modelos \textit{Transformers} \cite{Vaswani}.

\begin{table}[ht!]
    \begin{center}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{|l|l|l|l|}
        % \hline
        % \multicolumn{4}{|c|}{Recopilaciones} \\
        \hline
        \multirow{2}*{\textbf{Título}} & \multirow{2}*{\textbf{Año}} & \textbf{Métodos} & \multirow{2}*{\textbf{Descripción}}\\
         & & \textbf{cubiertos} & \\
        \hline
        \multirow{2}*{A survey of computer vision-based motion capture \cite{MOESLUND2001231}} & \multirow{2}*{2001} & \multirow{2}*{Tradicionales} &  Investigacion general sobre métodos de captura de movimientos basados en visión en\\
        & & & humanos. Incluye estimación de pose, seguimiento y reconocimiento de acciones. \\\hline

        A survey of advances in vision-based human motion capture and analysis \cite{MOESLUND200690} & 2006 & Tradicionales & Incluye una revisión de los métodos de captura de movimiento del año 2001 al 2006.\\ \hline

        \multirow{2}*{Vision-based human motion analysis: An overview. \cite{POPPE20074}} & \multirow{2}*{2007} & \multirow{2}*{Tradicionales} & Investigacion general sobre métodos de captura de movimientos usando datos \\
        & & & sin marcadores de dispositivos de captura. \\\hline

        \multirow{2}*{Advances in view-invariant human motion analysis: A review \cite{5191035}} & \multirow{2}*{2010} & \multirow{2}*{Tradicionales} & Estudio de métodos de estimación de pose en 3D, comportamiento y \\
        & & & reconocimiento/representación de acciones. \\ \hline

        Human pose estimation and activity recognition from multi-view videos: & \multirow{2}*{2012} & \multirow{2}*{Tradicionales} & Métodos de estimación de pose 3D y reconocimiento de acción usando datos \\
        Comparative explorations of recent developments \cite{6193117} &  &  & multi-vista\\ \hline

        A survey of human pose estimation: the body parts parsing based methods \cite{LIU201510} & 2015 & Tradicionales & Estudios de estimación de pose enfocados principalmente a las técnicas de localización \\
        & & & de las distintas partes del cuerpo. \\ \hline

        \multirow{2}*{Human pose estimation from monocular images: A comprehensive survey \cite{Gong2016}} & \multirow{2}*{2016} & \multirow{2}*{Ambos} & Enfocado en la estimación de pose usando datos monoculares incluyendo las\\
        & & & metodologías usadas en procesos tradicionales y basados en aprendizaje profundo. \\ \hline

        \multirow{2}*{3D human pose estimation: A review of the literature and analysis of covariates \cite{SARAFIANOS20161}} & \multirow{2}*{2016} & \multirow{2}*{Deep-Learning} & Revisión general del estado del arte de estimación de pose 3D usando imágenes\\
        & & & y videos RGB. \\ \hline

        \multirow{2}*{Monocular human pose estimation: a survey of deep learning-based methods \cite{CHEN2020102897}} & \multirow{2}*{2020} & \multirow{2}*{Deep-Learning} & Revisión y clasificación general de los métodos de estimación de pose basados en \\
        & & & aprendizaje profundo desde el 2014 usando solo datos monoculares.\\ \hline

        The progress of human pose estimation: a survey and taxonomy \cite{9144178} & \multirow{2}*{2020} & \multirow{2}*{Deep-Learning} & Revisión de los métodos basados en aprendizaje profundo para estimación de \\
        of models applied in 2D human pose estimation &  & & pose 2D\\ \hline

        Deep Learning-Based Human Pose Estimation: A Survey \cite{DBLP:journals/corr/abs-2012-13392} & 2020 & Deep-Learning & Estudio general del estado del arte de estimación de pose 2D y 3D.\\ \hline
    \end{tabular}}
    \end{center}
    \caption{Listado de diversas investigaciones de \textit{Estimación de Pose en Humanos} que abarcan
             tanto enfoques tradicionales como basados en aprendizaje profundo.
             Tabla basada en el trabajo de \citeauthor*{DBLP:journals/corr/abs-2012-13392}.}
    \label{Tab:hpe-survey}
\end{table}

\subsubsection{Taxonomía de Estimación de Pose 2D y 3D}


\begin{figure}\begin{center}
\begin{tikzpicture}[node distance=1.5cm,
        every node/.style={fill=white, font=\sffamily, scale=0.7}, align=center,
        >={Latex[width=2mm,length=2mm]},
        % Specifications for style of nodes:
        % base/.style = {rectangle, rounded corners, draw=black,
        %                 minimum width=4cm, minimum height=1cm,
        %                 text centered, font=\sffamily},
        % activityStarts/.style = {base, fill=blue!30},
        % startStop/.style = {base, fill=red!30},
        % activityRuns/.style = {base, fill=green!30},
        % process/.style = {base, minimum width=2.5cm, fill=blue!10, font=\ttfamily}
        ]
    % Specification of nodes (position, etc.)
    \node (HPE)          []              {HPE};
    \node (2D HPE)       [below of=HPE, left=6.5em of HPE]          {2D HPE};
    \node (3D HPE)       [below of=HPE, right=6.5em of HPE]   {3D HPE};

    \node (2D Single)    [below of= 2D HPE, left=1.0em of 2D HPE]          {2D Single};
    \node (2D Multiple)  [below of= 2D HPE, right=1.0em of 2D HPE]   {2D Multiple};

    \node (3D Single)    [below of= 3D HPE, left=1.0em of 3D HPE]          {3D Single};
    \node (3D Multiple)  [below of= 3D HPE, right=1.0em of 3D HPE]   {3D Multiple};

    \node (2D-Regresion) [below of= 2D Single, left=-1.5em of 2D Single]   {Regresion};
    \node (2D BPD)       [below of= 2D Single, right=-1.5em of 2D Single]   {Body Part \\ Detection};

    \node (2D-TD)        [below of= 2D Multiple, left=-1.5em of 2D Multiple]   {Top-Down};
    \node (2D-BU)        [below of= 2D Multiple, right=-1.5em of 2D Multiple]   {Bottom-Up};

    \node (3D-MF)        [below of= 3D Single, left=-1.5em of 3D Single]   {Model-Free};
    \node (3D-MB)        [below of= 3D Single, right=-1.5em of 3D Single]   {Model-Based};

    \node (3D-TD)        [below of= 3D Multiple, left=-1.5em of 3D Multiple]   {Top-Down};
    \node (3D-BU)        [below of= 3D Multiple, right=-1.5em of 3D Multiple]   {Bottom-Up};

    % \node (onResumeBlock)     [process, below of=onStartBlock]   {onResume()};
    % \node (activityRuns)      [activityRuns, below of=onResumeBlock]
    %                                                     {Activity is running};
    % \node (onPauseBlock)      [process, below of=activityRuns, yshift=-1cm]
    %                                                             {onPause()};
    % \node (onStopBlock)       [process, below of=onPauseBlock, yshift=-1cm]
    %                                                                 {onStop()};
    % \node (onDestroyBlock)    [process, below of=onStopBlock, yshift=-1cm]
    %                                                             {onDestroy()};
    % \node (onRestartBlock)    [process, right of=onStartBlock, xshift=4cm]
    %                                                             {onRestart()};
    % \node (ActivityEnds)      [startStop, left of=activityRuns, xshift=-4cm]
    %                                                     {Process is killed};
    % \node (ActivityDestroyed) [startStop, below of=onDestroyBlock]
    %                                                 {Activity is shut down};
    % Specification of lines between nodes specified above
    % with additional nodes for description
    \draw[-]             (HPE) -- (2D HPE);
    \draw[-]             (HPE) -- (3D HPE);
    \draw[-]             (2D HPE) -- (2D Single);
    \draw[-]             (2D HPE) -- (2D Multiple);
    \draw[-]             (3D HPE) -- (3D Single);
    \draw[-]             (3D HPE) -- (3D Multiple);
    \draw[-]             (2D Single) -- (2D-Regresion);
    \draw[-]             (2D Single) -- (2D BPD);
    \draw[-]             (2D Multiple) -- (2D-TD);
    \draw[-]             (2D Multiple) -- (2D-BU);
    \draw[-]             (3D Single) -- (3D-MF);
    \draw[-]             (3D Single) -- (3D-MB);
    \draw[-]             (3D Multiple) -- (3D-TD);
    \draw[-]             (3D Multiple) -- (3D-BU);

    % \draw[->]      (onStartBlock) -- (onResumeBlock);
    % \draw[->]     (onResumeBlock) -- (activityRuns);
    % \draw[->]      (activityRuns) -- node[text width=4cm]
    %                                 {Another activity comes in
    %                                 front of the activity} (onPauseBlock);
    % \draw[->]      (onPauseBlock) -- node {The activity is no longer visible}
    %                                 (onStopBlock);
    % \draw[->]       (onStopBlock) -- node {The activity is shut down by
    %                                 user or system} (onDestroyBlock);
    % \draw[->]    (onRestartBlock) -- (onStartBlock);
    % \draw[->]       (onStopBlock) -| node[yshift=1.25cm, text width=3cm]
    %                                 {The activity comes to the foreground}
    %                                 (onRestartBlock);
    % \draw[->]    (onDestroyBlock) -- (ActivityDestroyed);
    % \draw[->]      (onPauseBlock) -| node(priorityXMemory)
    %                                 {higher priority $\rightarrow$ more memory}
    %                                 (ActivityEnds);
    % \draw           (onStopBlock) -| (priorityXMemory);
    % \draw[->]     (ActivityEnds)  |- node [yshift=-2cm, text width=3.1cm]
    %                                 {User navigates back to the activity}
    %                                 (onCreateBlock);
    % \draw[->] (onPauseBlock.east) -- ++(2.6,0) -- ++(0,2) -- ++(0,2) --
    %     node[xshift=1.2cm,yshift=-1.5cm, text width=2.5cm]
    %     {The activity comes to the foreground}(onResumeBlock.east);
\end{tikzpicture}
\caption{M1 caption for diagram} \label{fig:HPE-diagram}
\end{center} \end{figure}

\textbf{Estimación de Pose en 2 y 3 dimensiones}

Existen dos grandes grupos que dividen las metodologías seguidas para la \textit{Estimación de Pose en
Humanos}; estimación de pose en 2 y 3 dimensiones. Cómo el nombre lo sugiere, la \textit{Estimación
de Pose 2D} (\textbf{2D HPE} por sus siglas en inglés) consiste en localizar articulaciones o partes del cuerpo
directamente en imágenes, por tanto, el marco de referencia de las posiciones de cada articulación es la
propia imagen. En la \textit{Estimación de Pose 3D} (\textbf{3D HPE}) las elementos detectados pasan a estar en 3D
y se busca un marco de referencia que mejor se ajuste las dimensiones espaciales de las
articulaciones y personas. Comúnmente se usa un cubo unitario cuyo centro corresponde a la
articulación que indica la cadera o \textit{hip} como se encuentra en la literatura, véase la figura XXX.

\textbf{Única o múltiples personas}.

En muchos escenarios las imágenes contienen más de una persona o se necesita hacer seguimiento
de multiples individuos. Por lo regular cuando aparecen más de un persona en una imagen
(\textbf{MPPE}, Multiple Person Pose Estimation)
se opta por identificar cada cuerpo en la imagen y posteriormente resolver individualmente
la estimación de pose para cada una de las entidades identificadas (\textbf{SPPE}, Single Pose Estimation).
La detección de los cuerpos se realiza en
etapas previas usando modelos de detección de objetos y entrenados para detectar cuerpos
humanos tales como \textit{MobileNet} \cite{DBLP:journals/corr/RenHG015}
\cite{DBLP:journals/corr/HowardZCKWWAA17} \cite{DBLP:journals/corr/abs-1801-04381} o \textit{YOLO}
(You Only Look Once) \cite{DBLP:journals/corr/RedmonDGF15} \cite{DBLP:journals/corr/abs-2004-10934}.

\textbf{Tipos de datos}.

En los últimos años, debido a la masificación
de dispositivos inteligentes, el acceso a una cámara digital no resulta un problema para la mayoria
de las personas, así, la mayoría de los trabajos realizados sobre Estimación de Pose usan
\textit{Imágenes RGB} debido a su alto uso, fácil captura y acceso.
Sin embargo, esto solo es cierto en el contexto de la Estimación de Pose 2D, puesto que el etiquetado de datos
usando las coordenadas de la imagen como referencia no representa demasiado problema. En la
Estimación de Pose 3D, la complejidad de la obtención de los datos se incrementa. Para llevar a cabo
el proceso de captura de los datos es necesario usar equipo especializado y costoso para la captura
de movimiento en acompañamiento de las cámaras de video \cite{6682899}. Existen dos tipos principales de captura
de movimiento, los ópticos y los no ópticos.
Generalmente, en los primeros el sujeto de prueba no necesita de aditamentos complejos como el uso de trajes
(exoesqueleto) y solo un software y cámaras con sensores se
encarga de registrar e interpretar el movimiento a un modelo digital. Si bien
aventajan en una reducción de coste su precisión es menor que los no ópticos.
El \textit{Kinect}, fabricado por \textit{Microsoft} es muy usado para generar imágenes de infrarrojos
\textit{IR-image} y su bajo conste en el mercado permite un fácil acceso a este dispositivo
\cite{6165146} \cite{Izadi11kinectfusion:real-time}.
Además dispositivos basados en tecnologías como \textit{LIDAR} para generar imágenes de profundad
cada vez son de más fácil acceso, bajo la comercialización de dispositivos inteligentes  por
compañías como Apple \cite{DBLP:journals/corr/abs-1711-06396} .


El segundo tipo se puede dividir en dos clases; los mecánicos los cuales usan giroscopios y acelerómetros para registrar el
movimiento y los electromagnéticos, cuyo funcionamiento es a través de la generación un campo
electromagnético y para posteriormente capturar las alteraciones en este que realiza el sujeto al moverse.

Depth (Time of Flight) image : In a Depth image, the value of a pixels relates to the distance from
the camera as measured by time-of-flight. The introduction and popularity of low-cost devices
like Microsoft Kinect has made it easier to obtain Depth data. Depth image can complement RGB image
to create more complex and accurate Computer Vision models, whereas Depth-only models are vastly
used where privacy is a concern.

Infra-red (IR) image : In an , the value of a pixel is determined by the amount of infrared
light reflected back to the camera. Experimentation in Computer Vision based on IR images are minimal,
as compared to RGB and Depth images. Microsoft Kinect also provides IR image while recording.
However, currently there are no datasets that contain IR images.

Static Image vs Video

A video is nothing but a collection of images, where every two consecutive frames share a huge
portion of the information present in them (which is the basis of most of the video compression
techniques). These temporal (time based) dependence in videos can be exploited while performing
Pose estimation.
For a video, a series of poses need to be produced for the input video sequence. It is expected
that the estimated poses should ideally be consistent across successive frames of video, and the
algorithm needs to be computationally efficient to handle large number of frames. The problem of
occlusion might be easier to solve for a video due to the availability of past or future frames
where the body part is not occluded.

If temporal features are not a part of the pipeline, it is possible to apply static pose estimation
for each frame in a video. However, the results are generally not as good as desired due to jitters
and inconsistency problems.

Body Model
Every pose estimation algorithm agrees upon a body model beforehand. It allows the algorithm to formalize the problem of human pose estimation into that of estimating the body model parameters. Most algorithms use a simple N-joint rigid kinematic skeleton model (N is typically between 13 to 30) as the final output. Formally, kinematic models can be represented as a graph, where each vertex V represents a joint. The edges E can encode constraints or prior beliefs about the structure of the body model.
Such a model suffices for most applications. However, for many other applications such as character animation, a more elaborate model may be needed. Some techniques have considered a highly detailed mesh models, representing the whole body with a point cloud.
Another rather primitive body model that was used in earlier Pose Estimation pipelines is a shape-based body model. In shape-based models, human body parts are approximated using geometric shapes like rectangles, cylinders, conics etc.

2D Pose Estimation vs 3D Pose Estimation
Most 3D Pose Estimation models first predict 2D Pose, and then try to lift it to 3D Pose. However,
some end-to-end 3D Pose Estimation techniques also exist which directly predict 3D Pose.

kinematic Model
Planar Model
Volumetric Model
-> https://viso.ai/deep-learning/pose-estimation-ultimate-overview/

Number of cameras
A major portion of research involves solving the pose estimation problem using input from a single camera. However, there are certain algorithms which try to use data from multiple viewpoints/cameras, combining them to generate more accurate poses and handle occlusions better. The research on multi-camera pose estimation is currently somewhat limited, primarily due to lack of good datasets.

Mapa de tipos de estimación de pose 3D
-> https://sci-hub.se/10.1016/j.cviu.2019.102897 pp 4

Why is it hard?
Strong articulations, small and barely visible joints, occlusions, clothing, and lighting changes make this a difficult problem.
Main challenges -> https://viso.ai/deep-learning/pose-estimation-ultimate-overview/

Pipelines

-> https://towardsdatascience.com/human-pose-estimation-simplified-6cfd88542ab3

Approaches

- Classical approaches
- Deep Learning based approaches
-> https://nanonets.com/blog/human-pose-estimation-2d-guide/
- Bottom-up
- Top-Down
-> https://viso.ai/deep-learning/pose-estimation-ultimate-overview/


Most popular pose estimation methods
-> https://viso.ai/deep-learning/pose-estimation-ultimate-overview/


datasets
-> https://sci-hub.se/10.1016/j.cviu.2016.09.002

Modern pose estimation algorithms are almost exclusively based on convolutional neural networks with hourglass architecture or its variants (see the image below). Such a network consists of two major parts: a convolutional encoder that compresses the input image into the so-called latent representation and decoder that constructs N heatmaps from the latent representation where N is the number of searched keypoints.

The major challenges for tracking poses from the car perspective are
(i) occlusions due to the viewing angle and (ii) prediction
speed to be able to react to real-time changes in the environ

Biased databases
La mayoría de la bases de datos son obtenidas a través de algunos pocos sujetos de prueba.
La estructura fisiológica de individuos no es perfecta y pueden variar debido a diversos factores
como el sexo, la raza, edad, lugar de nacimiento y desarrollo, enfermedades, factores genéticos, entre
otros, además de que los movimientos recreados entre sujetos no siempre son recreados de la misma manera
aunque las circunstancias o ambiente esté controlado.

Irregularidades en los datos reales.
Si los datos son obtenidos bajo un ambiente controlado es posible obtener imágenes fieles para el
entrenamiento. Por otro lodo, los modelos al ser desplegados y puestos en marcha en ambientes reales
se enfrentan a imprevistos de los que no se puede tener control; oclusiones de diversas partes del
cuerpo ya sea por el mismo sujeto o algún objeto extraño en la captura, movimientos extraños o rápidos
como correr o dar una patada donde el modelo no puede los puede identificar o el equipo de captura
no pueda obtener que se ven como fotogramas borrosos.

Monocular human pose estimation has some unique characteristics
and challenges:

Flexible body configuration indicates complex interdependent
joints and high degree-of-freedom limbs, which may cause self occlusions or rare/complex poses.
- Diverse body appearance includes different clothing and self similar parts.
- Complex environment may cause foreground occlusion, occlusion
or similar parts from nearby persons, various viewing angles, and
truncation in the camera vie


\subsection{Lung Pathologies detection}
