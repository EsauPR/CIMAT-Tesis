\subsection{Compuertas LSTM y GRU}

Hasta el momento, se ha hecho mención de las salidas $o^{(t)}$ y estados ocultos $h^{(t)}$ solo como
el resultado de operaciones aplicadas por dos funciones; $g$ y $f$ respectivamente. Existen varias
alternativas de construir una \textit{RNN}, una de las maneras más comunes es usando
\ref{eq:rnn_Ht} y \ref{eq:rnn_Ot}:

\begin{equation}
    h^{(t)} = \phi(x^{(t)} W_{x} + h^{(t-1)} W_h + b)
    \label{eq:rnn_Ht}
\end{equation}

\begin{equation}
    o^{(t)} = x^{(t)} W_{out} + b
    \label{eq:rnn_Ot}
\end{equation}

Los parámetros compartidos de la red ahora son descritos por las matrices
$W_x \in \mathbb{R}^{d \times k}$, $W_h \in \mathbb{R}^{k \times k}$ y $
W_{out} \in \mathbb{R}^{k \times q}$ con $k$ como la dimensión del estado oculto, $q$ la dimensión
de las salidas $o^{(t)}$, $b \in \mathbb{R} ^ {1 \times q}$ el parámetro de sesgo y $\phi$ es la
función de activación. De esta manera, los pesos de los parámetros aprendidos en la matriz $W_h$
determinan cómo será usada la información del pasado, codificada en $h^{(t-1)}$. Posteriormente,
es incluida a la codificación de la información del tiempo actual $t$ calculada por $W_x$. La figura
\ref{fig:rnn_cell} representa gráficamente la lógica usada para calcular los estados ocultos y las
salidas de la red.


\begin{figure}[ht!]
\centering
\includegraphics[width=0.4 \textwidth]{Chapters/2. Transformer/Figures/rnn/rnn_cell.jpg}
\caption{Cómputo del estado oculto y salida de una Red Neuronal Recurrente.}
\label{fig:rnn_cell}
\end{figure}

Sin embargo el cálculo de los estados ocultos mediante \ref{eq:rnn_Ht} presenta algunos problemas.
La interacción entre la información del pasado y la actual siempre es \quotes{plana}, es decir, la información
fluye a través del tiempo de la misma manera sin forma de dar prioridad o ignorar parte de
esta. Por lo que resulta una tarea un poco más complicada preservar información relevante a en cada paso
o desechar información que ya no es util para la red. También, causado por este mismo flujo de los
datos, la información del pasado poco a poco es opacada por nueva información, impidiendo que se
puedan encontrar dependencias de información en secuencias largas en tiempos distantes;
comúnmente se hace referencia a este problema como \textit{The Short-term Memory Problem} en inglés
\cite{VanishinGradient2}, aunado a problemas como el
\textit{Desvanecimiento o Explosión del Gradiente} \cite{VanishinGradient} \cite{pmlr-v28-pascanu13},
y acentuándose aun más
debido a las matrices de pesos compartidos en la recurrencia. Dichas
multiplicaciones de las matrices en la recurrencia tienen similitud a las realizadas en el
\textit{método de potencia}, en donde cualquier
componente en la matriz inicial que no esté alineada con el vector propio asociado al mayor valor
propio son eventualmente descartados \cite[pp.~390-392]{GoodBengCour16}). Por ende, los resultados
de este producto tendrán a ser cercanos a cero (desvanecerse) o explotar dependiendo de la magnitud
de la matriz de pesos.

Una manera de solventar los problemas anteriores son las \textbf{Redes Neuronales con Compuertas},
creadas con la idea de crear conexiones a través del tiempo de tal manera de tener gradientes que no
se desvanezcan o exploten, convirtiéndose además en un mecanismo para olvidar información pasada y
decidiendo automáticamente cuándo y cuánto de la información debe prevalecer.

\subsubsection{LSTM}

\textbf{Long Short-Term Memory}, \textbf{LSTM} por sus siglas en inglés, fue propuesta en 1997 por
\citeauthor{LSTM}, como un método de preservar dependencias de información relevante
distantes a corto plazo. Las \textit{LSTM} introducen un nuevo componente la
\textit{Celda de Memoria} cuya función es guardar información a través del tiempo y es
controlada por distintas compuertas. Las compuertas aprenden a distinguir que información es relevante y
cual no. Hay 3 de ellas, la \textit{Compuerta de Entrada}, la \textit{Compuerta de Olvido}
y la \textit{Compuerta de Salida}. La \textit{Compuerta de Entrada} $I^{(t)}$ (véase la figura
\ref{fig:rnn_lstm}) determina cuanta información actual debe ser contemplada a través de la
\textit{Memoria Candidata} $\tilde C^{(t)}$ para actualizar la \textit{Celda de Memoria} $C^{(t)}$.
La \textit{Compuerta de Olvido} $F^{(t)}$ indica qué información del pasado debe ser desechada de la
\textit{Celda de Memoria} $C^{(t-1)}$ y la \textit{Compuerta de Salida} ayuda a determinar el nuevo
estado $h^{(t)}$ via la \textit{Celda de Memoria} actual $C^{(t)}$.

\begin{figure}[ht!]
\centering
\includegraphics[width=1.0 \textwidth]{Chapters/2. Transformer/Figures/rnn/lstm.jpg}
\caption{LSTM. La \textit{Compuerta de Entrada} $I^{(t)}$ determina cuánta información actual debe
         ser considerada a través de la \textit{Memoria Candidata} $\tilde{C}^{(t)}$ para actualizar
         la \textit{Celda de Memoria} $C^{(t)}$. La \textit{Compuerta de Olvido} $F^{(t)}$ indica
         qué información del pasado debe ser desechada de la \textit{Celda de Memoria} $C^{(t-1)}$,
         y la \textit{Compuerta de Salida} $O^{(t)}$ ayuda a determinar el nuevo estado $h^{(t)}$ a
         través de la \textit{Celda de Memoria} actual $C^{(t)}$.}
\label{fig:rnn_lstm}
\end{figure}

Las ecuaciones \ref{eq:comp} rigen el comportamiento de Compuertas de Entrada, Salida y Olvido,

\begin{equation}
    \begin{split}
        I^{(t)} =  \sigma(x^{(t)} W_{xi} + h^{(t-1)} W_{hi} + b_i)\\
        F^{(t)} =  \sigma(x^{(t)} W_{xf} + h^{(t-1)} W_{hf} + b_f)\\
        O^{(t)} =  \sigma(x^{(t)} W_{xo} + h^{(t-1)} W_{ho} + b_o)\\
    \end{split}
    \label{eq:comp}
\end{equation}

\noindent donde $W_{xi}, W_{xf}, W_{xo} \in \mathbb{R}^{d \times k}$,
$W_{hi}, W_{hf}, W_{ho} \in \mathbb{R}^{k \times k}$ y $b_i, b_f, b_o \in \mathbb{R}^{1xk}$

La Memoria Candidata y la Celda de memoria son actualizadas mediante:

\begin{equation}
    \begin{split}
        \tilde C^{(t)} =  \tanh(x^{(t)} W_{xi} + h^{(t)} W_{hc} + b_c)\\
        C^{(t)} =  F^{(t)} \odot C^{(t-1)} + I^{(t)} \odot \tilde C^{(t)} \\
    \end{split}
\end{equation}

\noindent donde $W_{xi} \in \mathbb{R}^{d \times k}$,
$W_{hc} \in \mathbb{R}^{k \times k}$ y $b_c \in \mathbb{R}^{1xk}$

\noindent y finalmente el estado oculto $h^{(t)}$ esta dado por:

\begin{equation}
    \begin{split}
        h^{(t)} =  O^{(t)} \odot \tanh(C^{(t)}) \\
    \end{split}
\end{equation}

\noindent $\sigma$ y $\odot$ denotan la función de activación sigmoide y la multiplicación uno a uno
respectivamente.


\subsubsection{GRU}

\textbf{Gated Recurrent Units} o \textbf{GRU} por sus siglas en inglés, fueron propuestas en 2014
\cite{GRU1} como una alternativa computacionalmente más rápida y con similar rendimiento que las
\textit{LSTM} \cite{GRU2}. A diferencia de anteriores, las \textit{GRU} prescinden de la
\textit{Celda de Memoria} y utilizan un par de compuertas (la \textit{Compuerta de Actualización} y
la de \textit{Olvido}) para decidir que información aún es necesaria que esté codificada dentro del
estado oculto, véase la ecuación \ref{eq:gru_gates}.
La \textit{Compuerta de Olvido} permite decidir que del pasado aún debe ser transmitido a futuros
estados o de otro modo ser desechada. La \textit{Compuerta de Actualización} indica que información nueva es relevante y
necesita ser incorporada al no estar codificada dentro del estado oculto,
véase la ecuación \ref{eq:gru_out}.

\begin{equation}
    \begin{split}
        R^{(t)} = \sigma(x^{(t)} W_{xR} + h^{(t-1)} W_{hR} + b_R)\\
        Z^{(t)} = \sigma(x^{(t)} W_{xZ} + h^{(t-1)} W_{hZ} + b_Z)
    \end{split}
    \label{eq:gru_gates}
\end{equation}

\begin{equation}
    \begin{split}
        \tilde h^{(t)} = \tanh(x^{(t)} W_{xh} + ( R^{(t)} \odot h^{(t-1)}) W_{hh} + b_h)\\
        h^{(t)} =  Z^{(t)} \odot h^{(t-1)} + (1 - Z^{(t)}) \odot \tilde h^{(t-1)} \\
    \end{split}
    \label{eq:gru_out}
\end{equation}

\begin{figure}[ht!]
\centering
\includegraphics[width=1.0 \textwidth]{Chapters/2. Transformer/Figures/rnn/GRU.jpg}
\caption{GRU. A diferencia de las \textit{LSTM}, las \textit{GRU} prescinden de la
         \textit{Celda de Memoria} y utilizan un par de compuertas (la \textit{Compuerta de
         Actualización} y la \textit{Compuerta de Olvido}) para decidir qué información es necesaria
         que esté codificada dentro del estado oculto.}
\label{fig:rnn_gru}
\end{figure}
