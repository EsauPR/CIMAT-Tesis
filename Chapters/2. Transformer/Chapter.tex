\section{De RNN's a Transformers}

Las \textbf{Recurrent Neural Networks, RNN} (Redes Neuronales Recurrentes) basadas en el
trabajo de \citeauthor{Rumelhart} datan del año 1986. Este tipo de redes están especializadas
en el procesamiento de datos que contienen información temporal, mejorando los resultados obtenidos
por otros tipos de redes como \textit{FeedForward Neural Networks} (Redes neuronales de propagación
hacia adelante) o \textit{Convolutional Neural Networks} (Redes neuronales convolucionales).

La idea principal detrás de estos modelos de redes es el concepto de \textit{parámetros compartidos}
encontrado en la literatura de idioma ingles como \textit{Parameter Sharing}.
Usando parámetros compartidos, un modelo puede generalizar mejor cuando la información
está contenida en diferentes partes de una secuencia. Así, el modelo no necesita aprender
independientemente todas las reglas que forman las secuencias, sino que ahora, la salida para cada
elemento perteneciente a un tiempo $t$ está determinada por la salida del elemento anterior $t-1$.
Esto resulta en una recurrencia con las mismas reglas de actualización aplicadas a cada elemento en el tiempo.
La ecuación \ref{eq:rnnh} representa este proceso; $h^{(t)}$ es el estado de la recurrencia definida
por una función $f$ sobre un elemento $x^{(t)}$ de la secuencia $X$ en el tiempo $t$ y $\theta$ son
los parámetros compartidos.

\begin{equation}
    h^{(t)} = f(x^{(t)}, h^{(t-1)}; \theta)
    \label{eq:rnnh}
\end{equation}

En una \textit{RNN} vista como un \textit{grafo computacional dirigido y acíclico}, cada nodo
representa un estado en la recurrencia y procesa la información de la secuencia $X$ con los mismos
parámetros $\theta$ en cada paso. Observe la figura \ref{fig:rnn_cg}.

\begin{figure}[!ht]
\centering
\includegraphics[width=.8\textwidth]{Chapters/2. Transformer/Figures/rnn/rnn_cgraph.png}
\caption{Grafo computacional generado por una \textit{RNN} al
        \quotes{desenrollar} la recurrencia. Usando los parámetros compartidos en cada nodo
        y con cada elemento $x^{(t)}$ de la secuencia genera un nuevo estado oculto $h^{(t)}$
        para retroalimentar nuevamente la entrada del siguiente nodo.}
\label{fig:rnn_cg}
\end{figure}

% Redes Neuronales Recurrentes más comunes
\input{Chapters/2. Transformer/Chapter_1}

% Compuertas LSTM y GRU
\input{Chapters/2. Transformer/Chapter_2}

% Modelos de atención
\input{Chapters/2. Transformer/Chapter_3}

% Transformer
\input{Chapters/2. Transformer/Chapter_4}

% \begin{figure}
%     \begin{center}
%         \scalebox{0.6}{\input{ROC_AUC.pgf}}
%     \end{center}
% \end{figure}
