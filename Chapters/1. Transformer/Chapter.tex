\section{De RNN's a Transformers}

Las \textbf{Redes Neuronales Recurrentes} o \textbf{RNN} (por sus siglas en Inglés) datan del año
1986, basadas en el trabajo de Rumelhart \cite{Rumelhart}. Este tipo de redes están especializadas
en el procesamiento de datos que contienen información temporal, mejorando los resultados obtenidos
por otros tipos de redes como \textit{Redes FeedForward} o \textit{Redes Convolucionales}.

La principal idea detrás de estos modelos de red es el concepto de \textit{Parameter Sharing}.
Con \textit{Parameter Sharing} un modelo un modelo puede generalizar mejor cuando la información
que esta contenida en diferentes partes de una secuencia. Así, el modelo no necesita aprender
independientemente todas las reglas que forman la secuencias, sino que ahora, la salida para cada
elemento en el tiempo esta determinada por la salida del elemento anterior. Resultando en una
recurrencia con las mismas reglas de actualización aplicadas a cada elemento en el tiempo.
La ecuación \ref{eq:rnnh} representa este proceso; $h^{(t)}$ es el estado de la recurrencia aplicada
por alguna función $f$ a un elemento $x^{(t)}t$ de la secuencia $x$ en el tiempo, $t$ y $\theta$ son
los parámetros compartidos.

\begin{equation}
    h^{(t)} = f(x^{(t)}, h^{(t-1)}; \theta)
    \label{eq:rnnh}
\end{equation}

En una \textit{RNN} vista como un \textit{gráfo computacional dirigído y acíclico}, cada nodo
representa un estado en la recurrencia y procesa la información de la secuencia $x$ con los mismos
parámetros $\theta$ en cada paso, observe la figura \ref{fig:rnn_cg}.

\begin{figure}[!ht]
\centering
\includegraphics[width=.8\textwidth]{Chapters/1. Transformer/Figures/rnn/rnn_cgraph.png}
\caption[RNN - Grafo Computacional]{Grafo computacional generado por una \textit{RNN} al "desenrollar" la
recurrencia. Usando los parámetros compartidos en cada nodo, con cada elemento $x^{(t)}$ de la
secuencia genera un nuevo estado oculto $h^{(t)}$ para retroalimentar nuevamente la entrada del
siguiente nodo.}
\label{fig:rnn_cg}
\end{figure}

% Redes Neuronales Recurrentes más comunes
\input{Chapters/1. Transformer/Chapter_1}

% Compuertas LSTM y GRU
\input{Chapters/1. Transformer/Chapter_2}

% Modelos de atención
\input{Chapters/1. Transformer/Chapter_3}

% Transformer
\input{Chapters/1. Transformer/Chapter_4}

% \begin{figure}
%     \begin{center}
%         \scalebox{0.6}{\input{ROC_AUC.pgf}}
%     \end{center}
% \end{figure}
