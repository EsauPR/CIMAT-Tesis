% This happens to be the bibliography from the thesis of Cole Meisenhelder, '15

% To actually cite articles in your document you will use the \cite{}; the first item in each of the entries below is the identifier used to call these articles, i.e. \cite{Meisenhelder}

% Citation information can be obtained from NASA ADS. After you click on the abstract, select Export Citation, and use the BibTex format.

%% Saved with string encoding Unicode (UTF-8)

@article{Rumelhart,
	author={Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	title={Learning representations by back-propagating errors},
	journal={Nature},
	year={1986},
	month={Oct},
	day={01},
	volume={323},
	number={6088},
	pages={533-536},
	abstract={We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal `hidden' units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
	issn={1476-4687},
	doi={10.1038/323533a0},
	url={https://doi.org/10.1038/323533a0}
}

@article{Vaswani,
	author    = {Ashish Vaswani and
               Noam Shazeer and
               Niki Parmar and
               Jakob Uszkoreit and
               Llion Jones and
               Aidan N. Gomez and
               Lukasz Kaiser and
               Illia Polosukhin},
	title     = {Attention Is All You Need},
	journal   = {CoRR},
	volume    = {abs/1706.03762},
	year      = {2017},
	url       = {http://arxiv.org/abs/1706.03762},
	archivePrefix = {arXiv},
	eprint    = {1706.03762},
	timestamp = {Sat, 23 Jan 2021 01:20:40 +0100},
	biburl    = {https://dblp.org/rec/journals/corr/VaswaniSPUJGKP17.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Schuster,
	author={Schuster, M. and Paliwal, K.K.},
	journal={IEEE Transactions on Signal Processing},
	title={Bidirectional recurrent neural networks},
	year={1997},
	volume={45},
	number={11},
	pages={2673-2681},
	doi={10.1109/78.650093}
}

@article{LSTM,
	author = {Hochreiter, Sepp and Schmidhuber, JÃ¼rgen},
    title = "{Long Short-Term Memory}",
    journal = {Neural Computation},
    volume = {9},
    number = {8},
    pages = {1735-1780},
    year = {1997},
    month = {11},
    abstract = "{Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.}",
    issn = {0899-7667},
    doi = {10.1162/neco.1997.9.8.1735},
    url = {https://doi.org/10.1162/neco.1997.9.8.1735},
    eprint = {https://direct.mit.edu/neco/article-pdf/9/8/1735/813796/neco.1997.9.8.1735.pdf},
}

@inbook{VanishinGradient,
	author={Kolen, John F. and Kremer, Stefan C.},
	booktitle={A Field Guide to Dynamical Recurrent Networks},
	title={Gradient Flow in Recurrent Nets: The Difficulty of Learning LongTerm Dependencies},
	year={2001},
	volume={},
	number={},
	pages={237-243},
	doi={10.1109/9780470544037.ch14}
}

@article{VanishinGradient2,
	author={Bengio, Y. and Simard, P. and Frasconi, P.},
	journal={IEEE Transactions on Neural Networks},
	title={Learning long-term dependencies with gradient descent is difficult},
	year={1994},
	volume={5},
	number={2},
	pages={157-166},
	doi={10.1109/72.279181}
}


@Book{GoodBengCour16,
	Title = {Deep Learning},
	Author = {Ian J. Goodfellow and Yoshua Bengio and Aaron Courville},
	Publisher = {MIT Press},
	Year = {2017},
	Address = {Cambridge, MA},
	Note = {\url{http://www.deeplearningbook.org}}
}

@article{GRU1,
	author    = {KyungHyun Cho and
				Bart van Merrienboer and
				Dzmitry Bahdanau and
				Yoshua Bengio},
	title     = {On the Properties of Neural Machine Translation: Encoder-Decoder Approaches},
	journal   = {CoRR},
	volume    = {abs/1409.1259},
	year      = {2014},
	url       = {http://arxiv.org/abs/1409.1259},
	archivePrefix = {arXiv},
	eprint    = {1409.1259},
	timestamp = {Mon, 13 Aug 2018 16:47:23 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/ChoMBB14.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{GRU2,
	author    = {Junyoung Chung and
				{\c{C}}aglar G{\"{u}}l{\c{c}}ehre and
				KyungHyun Cho and
				Yoshua Bengio},
	title     = {Empirical Evaluation of Gated Recurrent Neural Networks on Sequence
				Modeling},
	journal   = {CoRR},
	volume    = {abs/1412.3555},
	year      = {2014},
	url       = {http://arxiv.org/abs/1412.3555},
	archivePrefix = {arXiv},
	eprint    = {1412.3555},
	timestamp = {Mon, 13 Aug 2018 16:47:38 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/ChungGCB14.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{bahdanau2016neural,
	title={Neural Machine Translation by Jointly Learning to Align and Translate},
	author={Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
	year={2016},
	eprint={1409.0473},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}


@article{DBLP:journals/corr/GravesWD14,
	Author    = {Alex Graves and
				Greg Wayne and
				Ivo Danihelka},
	title     = {Neural Turing Machines},
	journal   = {CoRR},
	volume    = {abs/1410.5401},
	year      = {2014},
	url       = {http://arxiv.org/abs/1410.5401},
	archivePrefix = {arXiv},
	eprint    = {1410.5401},
	timestamp = {Mon, 13 Aug 2018 16:46:28 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/GravesWD14.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/LuongPM15,
	author    = {Minh{-}Thang Luong and
				Hieu Pham and
				Christopher D. Manning},
	title     = {Effective Approaches to Attention-based Neural Machine Translation},
	journal   = {CoRR},
	volume    = {abs/1508.04025},
	year      = {2015},
	url       = {http://arxiv.org/abs/1508.04025},
	archivePrefix = {arXiv},
	eprint    = {1508.04025},
	timestamp = {Mon, 13 Aug 2018 16:46:14 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/LuongPM15.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/VaswaniSPUJGKP17,
	author    = {Ashish Vaswani and
				Noam Shazeer and
				Niki Parmar and
				Jakob Uszkoreit and
				Llion Jones and
				Aidan N. Gomez and
				Lukasz Kaiser and
				Illia Polosukhin},
	title     = {Attention Is All You Need},
	journal   = {CoRR},
	volume    = {abs/1706.03762},
	year      = {2017},
	url       = {http://arxiv.org/abs/1706.03762},
	archivePrefix = {arXiv},
	eprint    = {1706.03762},
	timestamp = {Sat, 23 Jan 2021 01:20:40 +0100},
	biburl    = {https://dblp.org/rec/journals/corr/VaswaniSPUJGKP17.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{weng2018attention,
	title   = "Attention? Attention!",
	author  = "Weng, Lilian",
	journal = "lilianweng.github.io/lil-log",
	year    = "2018",
	url     = "http://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html"
}

@article{DBLP:journals/corr/ChoMGBSB14,
	author    = {Kyunghyun Cho and
				Bart van Merrienboer and
				{\c{C}}aglar G{\"{u}}l{\c{c}}ehre and
				Fethi Bougares and
				Holger Schwenk and
				Yoshua Bengio},
	title     = {Learning Phrase Representations using {RNN} Encoder-Decoder for Statistical
				Machine Translation},
	journal   = {CoRR},
	volume    = {abs/1406.1078},
	year      = {2014},
	url       = {http://arxiv.org/abs/1406.1078},
	archivePrefix = {arXiv},
	eprint    = {1406.1078},
	timestamp = {Mon, 13 Aug 2018 16:46:44 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/ChoMGBSB14.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/SordoniBB16,
	author    = {Alessandro Sordoni and
				Philip Bachman and
				Yoshua Bengio},
	title     = {Iterative Alternating Neural Attention for Machine Reading},
	journal   = {CoRR},
	volume    = {abs/1606.02245},
	year      = {2016},
	url       = {http://arxiv.org/abs/1606.02245},
	archivePrefix = {arXiv},
	eprint    = {1606.02245},
	timestamp = {Mon, 22 Jul 2019 13:00:16 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/SordoniBB16.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1709-00893,
	author    = {Dehong Ma and
				Sujian Li and
				Xiaodong Zhang and
				Houfeng Wang},
	title     = {Interactive Attention Networks for Aspect-Level Sentiment Classification},
	journal   = {CoRR},
	volume    = {abs/1709.00893},
	year      = {2017},
	url       = {http://arxiv.org/abs/1709.00893},
	archivePrefix = {arXiv},
	eprint    = {1709.00893},
	timestamp = {Mon, 23 Nov 2020 08:36:40 +0100},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1709-00893.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2009-14794,
	author    = {Krzysztof Choromanski and
				Valerii Likhosherstov and
				David Dohan and
				Xingyou Song and
				Andreea Gane and
				Tam{\'{a}}s Sarl{\'{o}}s and
				Peter Hawkins and
				Jared Davis and
				Afroz Mohiuddin and
				Lukasz Kaiser and
				David Belanger and
				Lucy J. Colwell and
				Adrian Weller},
	title     = {Rethinking Attention with Performers},
	journal   = {CoRR},
	volume    = {abs/2009.14794},
	year      = {2020},
	url       = {https://arxiv.org/abs/2009.14794},
	archivePrefix = {arXiv},
	eprint    = {2009.14794},
	timestamp = {Wed, 23 Jun 2021 10:58:18 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-2009-14794.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1810-10126,
	author    = {Yang Li and
				Lukasz Kaiser and
				Samy Bengio and
				Si Si},
	title     = {Area Attention},
	journal   = {CoRR},
	volume    = {abs/1810.10126},
	year      = {2018},
	url       = {http://arxiv.org/abs/1810.10126},
	archivePrefix = {arXiv},
	eprint    = {1810.10126},
	timestamp = {Thu, 26 Sep 2019 08:29:12 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1810-10126.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Pavlopoulos,
	author = {Pavlopoulos, John and Malakasiotis, Prodromos and Androutsopoulos, Ion},
	year = {2017},
	month = {01},
	pages = {1125-1135},
	title = {Deeper Attention to Abusive User Content Moderation},
	doi = {10.18653/v1/D17-1117}
}

@InProceedings{pmlr-v28-pascanu13,
	title = {On the difficulty of training recurrent neural networks},
	author = {Razvan Pascanu and Tomas Mikolov and Yoshua Bengio},
	booktitle = {Proceedings of the 30th International Conference on Machine Learning},
	pages = {1310--1318},
	year = {2013},
	editor = {Sanjoy Dasgupta and David McAllester},
	volume = {28(3)},
	series = {Proceedings of Machine Learning Research},
	address = {Atlanta, Georgia, USA},
	month = {17--19 Jun}
}

@article{DBLP:journals/corr/abs-1904-02874,
	author    = {Sneha Chaudhari and
				Gungor Polatkan and
				Rohan Ramanath and
				Varun Mithal},
	title     = {An Attentive Survey of Attention Models},
	journal   = {CoRR},
	volume    = {abs/1904.02874},
	year      = {2019},
	url       = {http://arxiv.org/abs/1904.02874},
	archivePrefix = {arXiv},
	eprint    = {1904.02874},
	timestamp = {Wed, 24 Apr 2019 12:21:25 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1904-02874.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/MartinsA16,
	author    = {Andr{\'{e}} F. T. Martins and
				Ram{\'{o}}n Fernandez Astudillo},
	title     = {From Softmax to Sparsemax: {A} Sparse Model of Attention and Multi-Label
				Classification},
	journal   = {CoRR},
	volume    = {abs/1602.02068},
	year      = {2016},
	url       = {http://arxiv.org/abs/1602.02068},
	archivePrefix = {arXiv},
	eprint    = {1602.02068},
	timestamp = {Mon, 26 Oct 2020 15:47:01 +0100},
	biburl    = {https://dblp.org/rec/journals/corr/MartinsA16.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2006-07214,
	author    = {Andr{\'{e}} F. T. Martins and
				Marcos V. Treviso and
				Ant{\'{o}}nio Farinhas and
				Vlad Niculae and
				M{\'{a}}rio A. T. Figueiredo and
				Pedro M. Q. Aguiar},
	title     = {Sparse and Continuous Attention Mechanisms},
	journal   = {CoRR},
	volume    = {abs/2006.07214},
	year      = {2020},
	url       = {https://arxiv.org/abs/2006.07214},
	archivePrefix = {arXiv},
	eprint    = {2006.07214},
	timestamp = {Wed, 17 Jun 2020 14:28:54 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-2006-07214.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{NEURIPS2019_16fc18d7,
	author = {Tay, Yi and Luu, Anh Tuan and Zhang, Aston and Wang, Shuohang and Hui, Siu Cheung},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
	pages = {},
	publisher = {Curran Associates, Inc.},
	title = {Compositional De-Attention Networks},
	url = {https://proceedings.neurips.cc/paper/2019/file/16fc18d787294ad5171100e33d05d4e2-Paper.pdf},
	volume = {32},
	year = {2019}
}

@InProceedings{yang2016hierarchical,
	author = {Yang, Zichao and Yang, Diyi and Dyer, Chris and He, Xiaodong and Smola, Alex and Hovy, Eduard},
	title = {Hierarchical Attention Networks for Document Classification},
	booktitle = {NAACL 2016},
	year = {2016},
	month = {June},
	abstract = {We propose a hierarchical attention network for document classification. Our model has two distinctive characteristics: (i) it has a hierarchical structure that mirrors the hierarchical structure of documents; (ii) it has two levels of attention mechanisms applied at the wordand sentence-level, enabling it to attend differentially to more and less important content when constructing the document representation. Experiments conducted on six large scale text classification tasks demonstrate that the proposed architecture outperform previous methods by a substantial margin. Visualization of the attention layers illustrates that the model selects qualitatively informative words and sentences.},
	url = {https://www.microsoft.com/en-us/research/publication/hierarchical-attention-networks-document-classification/},
	pages = {1480-1489},
	edition = {NAACL 2016},
}

@article{DBLP:journals/corr/XuBKCCSZB15,
  author    = {Kelvin Xu and
               Jimmy Ba and
               Ryan Kiros and
               Kyunghyun Cho and
               Aaron C. Courville and
               Ruslan Salakhutdinov and
               Richard S. Zemel and
               Yoshua Bengio},
  title     = {Show, Attend and Tell: Neural Image Caption Generation with Visual
               Attention},
  journal   = {CoRR},
  volume    = {abs/1502.03044},
  year      = {2015},
  url       = {http://arxiv.org/abs/1502.03044},
  archivePrefix = {arXiv},
  eprint    = {1502.03044},
  timestamp = {Mon, 13 Aug 2018 16:47:52 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/XuBKCCSZB15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/GehringAGYD17,
  author    = {Jonas Gehring and
               Michael Auli and
               David Grangier and
               Denis Yarats and
               Yann N. Dauphin},
  title     = {Convolutional Sequence to Sequence Learning},
  journal   = {CoRR},
  volume    = {abs/1705.03122},
  year      = {2017},
  url       = {http://arxiv.org/abs/1705.03122},
  archivePrefix = {arXiv},
  eprint    = {1705.03122},
  timestamp = {Mon, 13 Aug 2018 16:48:03 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/GehringAGYD17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1908-03265,
  author    = {Liyuan Liu and
               Haoming Jiang and
               Pengcheng He and
               Weizhu Chen and
               Xiaodong Liu and
               Jianfeng Gao and
               Jiawei Han},
  title     = {On the Variance of the Adaptive Learning Rate and Beyond},
  journal   = {CoRR},
  volume    = {abs/1908.03265},
  year      = {2019},
  url       = {http://arxiv.org/abs/1908.03265},
  archivePrefix = {arXiv},
  eprint    = {1908.03265},
  timestamp = {Mon, 19 Aug 2019 13:21:03 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1908-03265.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{DBLP:journals/corr/abs-1910-04209,
  author    = {Jerry Ma and
               Denis Yarats},
  title     = {On the adequacy of untuned warmup for adaptive optimization},
  journal   = {CoRR},
  volume    = {abs/1910.04209},
  year      = {2019},
  url       = {http://arxiv.org/abs/1910.04209},
  archivePrefix = {arXiv},
  eprint    = {1910.04209},
  timestamp = {Wed, 16 Oct 2019 16:25:53 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1910-04209.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1804-00247,
  author    = {Martin Popel and
               Ondrej Bojar},
  title     = {Training Tips for the Transformer Model},
  journal   = {CoRR},
  volume    = {abs/1804.00247},
  year      = {2018},
  url       = {http://arxiv.org/abs/1804.00247},
  archivePrefix = {arXiv},
  eprint    = {1804.00247},
  timestamp = {Mon, 13 Aug 2018 16:47:13 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1804-00247.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@InProceedings{pmlr-v119-huang20f,
  title = 	 {Improving Transformer Optimization Through Better Initialization},
  author =       {Huang, Xiao Shi and Perez, Felipe and Ba, Jimmy and Volkovs, Maksims},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {4475--4483},
  year = 	 {2020},
  editor = 	 {III, Hal DaumÃ© and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/huang20f/huang20f.pdf},
  url = 	 {https://proceedings.mlr.press/v119/huang20f.html},
  abstract = 	 {The Transformer architecture has achieved considerable success recently; the key component of the Transformer is the attention layer that enables the model to focus on important regions within an input sequence. Gradient optimization with attention layers can be notoriously difficult requiring tricks such as learning rate warmup to prevent divergence. As Transformer models are becoming larger and more expensive to train, recent research has focused on understanding and improving optimization in these architectures. In this work our contributions are two-fold: we first investigate and empirically validate the source of optimization problems in the encoder-decoder Transformer architecture; we then propose a new weight initialization scheme with theoretical justification, that enables training without warmup or layer normalization. Empirical results on public machine translation benchmarks show that our approach achieves leading accuracy, allowing to train deep Transformer models with 200 layers in both encoder and decoder (over 1000 attention/MLP blocks) without difficulty. Code for this work is available here:&nbsp;\url{https://github.com/layer6ai-labs/T-Fixup}.}
}

@article{DBLP:journals/corr/abs-1711-02132,
  author    = {Karim Ahmed and
               Nitish Shirish Keskar and
               Richard Socher},
  title     = {Weighted Transformer Network for Machine Translation},
  journal   = {CoRR},
  volume    = {abs/1711.02132},
  year      = {2017},
  url       = {http://arxiv.org/abs/1711.02132},
  archivePrefix = {arXiv},
  eprint    = {1711.02132},
  timestamp = {Mon, 13 Aug 2018 16:48:34 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1711-02132.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2002-04745,
  author    = {Ruibin Xiong and
               Yunchang Yang and
               Di He and
               Kai Zheng and
               Shuxin Zheng and
               Chen Xing and
               Huishuai Zhang and
               Yanyan Lan and
               Liwei Wang and
               Tie{-}Yan Liu},
  title     = {On Layer Normalization in the Transformer Architecture},
  journal   = {CoRR},
  volume    = {abs/2002.04745},
  year      = {2020},
  url       = {https://arxiv.org/abs/2002.04745},
  archivePrefix = {arXiv},
  eprint    = {2002.04745},
  timestamp = {Sat, 23 Jan 2021 01:14:26 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2002-04745.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1804-09849,
  author    = {Mia Xu Chen and
               Orhan Firat and
               Ankur Bapna and
               Melvin Johnson and
               Wolfgang Macherey and
               George F. Foster and
               Llion Jones and
               Niki Parmar and
               Mike Schuster and
               Zhifeng Chen and
               Yonghui Wu and
               Macduff Hughes},
  title     = {The Best of Both Worlds: Combining Recent Advances in Neural Machine
               Translation},
  journal   = {CoRR},
  volume    = {abs/1804.09849},
  year      = {2018},
  url       = {http://arxiv.org/abs/1804.09849},
  archivePrefix = {arXiv},
  eprint    = {1804.09849},
  timestamp = {Fri, 20 Mar 2020 09:13:26 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1804-09849.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{DBLP:journals/corr/abs-1802-05751,
  author    = {Niki Parmar and
               Ashish Vaswani and
               Jakob Uszkoreit and
               Lukasz Kaiser and
               Noam Shazeer and
               Alexander Ku},
  title     = {Image Transformer},
  journal   = {CoRR},
  volume    = {abs/1802.05751},
  year      = {2018},
  url       = {http://arxiv.org/abs/1802.05751},
  archivePrefix = {arXiv},
  eprint    = {1802.05751},
  timestamp = {Mon, 13 Aug 2018 16:48:00 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1802-05751.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1904-10509,
  author    = {Rewon Child and
               Scott Gray and
               Alec Radford and
               Ilya Sutskever},
  title     = {Generating Long Sequences with Sparse Transformers},
  journal   = {CoRR},
  volume    = {abs/1904.10509},
  year      = {2019},
  url       = {http://arxiv.org/abs/1904.10509},
  archivePrefix = {arXiv},
  eprint    = {1904.10509},
  timestamp = {Thu, 02 May 2019 15:13:44 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1904-10509.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1905-07799,
  author    = {Sainbayar Sukhbaatar and
               Edouard Grave and
               Piotr Bojanowski and
               Armand Joulin},
  title     = {Adaptive Attention Span in Transformers},
  journal   = {CoRR},
  volume    = {abs/1905.07799},
  year      = {2019},
  url       = {http://arxiv.org/abs/1905.07799},
  archivePrefix = {arXiv},
  eprint    = {1905.07799},
  timestamp = {Tue, 28 May 2019 12:48:08 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1905-07799.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{DBLP:journals/corr/abs-1901-02860,
  author    = {Zihang Dai and
               Zhilin Yang and
               Yiming Yang and
               Jaime G. Carbonell and
               Quoc V. Le and
               Ruslan Salakhutdinov},
  title     = {Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context},
  journal   = {CoRR},
  volume    = {abs/1901.02860},
  year      = {2019},
  url       = {http://arxiv.org/abs/1901.02860},
  eprinttype = {arXiv},
  eprint    = {1901.02860},
  timestamp = {Fri, 01 Feb 2019 13:39:59 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1901-02860.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2004-05150,
  author    = {Iz Beltagy and
               Matthew E. Peters and
               Arman Cohan},
  title     = {Longformer: The Long-Document Transformer},
  journal   = {CoRR},
  volume    = {abs/2004.05150},
  year      = {2020},
  url       = {https://arxiv.org/abs/2004.05150},
  eprinttype = {arXiv},
  eprint    = {2004.05150},
  timestamp = {Tue, 14 Apr 2020 16:40:34 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2004-05150.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1906-11024,
  author    = {Tong Xiao and
               Yinqiao Li and
               Jingbo Zhu and
               Zhengtao Yu and
               Tongran Liu},
  title     = {Sharing Attention Weights for Fast Transformer},
  journal   = {CoRR},
  volume    = {abs/1906.11024},
  year      = {2019},
  url       = {http://arxiv.org/abs/1906.11024},
  eprinttype = {arXiv},
  eprint    = {1906.11024},
  timestamp = {Mon, 08 Jul 2019 16:13:41 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1906-11024.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
